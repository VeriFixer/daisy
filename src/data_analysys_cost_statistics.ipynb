{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb0bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.get_dataframe_from_results import get_pandas_dataset\n",
    "from analysis.dataset_graphs import plot_graphs_of_dataset_loc\n",
    "from analysis.get_tables_results import create_table_cleaned\n",
    "from analysis.get_results import bar_chart_fix_position_cleaned\n",
    "from analysis.get_results import sucess_vs_position_cleaned,get_latex_table_with_verif_stats\n",
    "import utils.global_variables as gl\n",
    "import llm.llm_configurations as llm_configurations\n",
    "import llm.llm_pipeline as llm_pipeline\n",
    "from llm.llm_create import create_llm\n",
    "\n",
    "import utils.global_variables as gl\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88765ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = gl.BASE_PATH / \"results/dafny_llm_results_cost_statistics\"\n",
    "shutil.rmtree(RESULT_DIR, ignore_errors=True)\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_DIR = gl.DAFNY_ASSERTION_DATASET\n",
    "\n",
    "base_prompt = gl.BASE_PROMPT\n",
    "localization_base_prompt = gl.LOCALIZATION_BASE_PROMPT\n",
    "system_prompt = gl.SYSTEM_PROMPT\n",
    "# Test point for the test dataset and for the llm_results_dir_Test\n",
    "global_options = llm_pipeline.GlobalOptions(\n",
    "    dafny_exec= gl.DAFNY_EXEC,\n",
    "    temp_dir= gl.TEMP_FOLDER,\n",
    "    llm_results_dir=RESULT_DIR,\n",
    "    assertion_dataset_path=DATASET_DIR\n",
    ")\n",
    "\n",
    "run_options = llm_pipeline.RunOptions(\n",
    "    number_assertions_to_test=-1, # if -1 test all assetions\n",
    "    number_rounds=1, # number of indepedent rounds in each assertion\n",
    "    number_retries_chain=1, #number of retries it tries to ask fix for previous wrong andwer\n",
    "    add_error_message = True,\n",
    "    remove_empty_lines=True,\n",
    "    change_assertion_per_text=\"\", #Assertion with oracle to retrieve position\n",
    "    base_prompt=base_prompt,\n",
    "    localization_base_prompt=localization_base_prompt,\n",
    "    examples_to_augment_prompt_type = llm_pipeline.ExampleStrategies.NONE, # \"RANDOM\", \"DYNAMIC\", \"NONE\", \"EMBEDDED\" \"TFIDF\" all options    \n",
    "                                               # DYNAMIC represents my code/error message embeddings\n",
    "    examples_weight_of_error_message = 0.5, # Wieght of the erro message in case of DYNAMIC in relantion with code                                 \n",
    "    number_examples_to_add=3,\n",
    "    # Fields ignore for all except LLM_EXAMPLE\n",
    "    examples_to_augment_prompt_type_pos= llm_pipeline.ExampleStrategies.NONE, # \"RANDOM\", \"DYNAMIC\", \"NONE\", \"EMBEDDED\" \"TFIDF\" all options    \n",
    "                                               # DYNAMIC represents my code/error message embeddings\n",
    "    examples_weight_of_error_message_pos = 0.5, # Wieght of the erro message in case of DYNAMIC in relantion with code                                 \n",
    "    number_examples_to_add_pos=3,\n",
    "\n",
    "    limit_example_length_bytes = 1200, # Option Not implemented\n",
    "    verifier_output_filter_warnings = True, # If 1 only errors are passed in the verifier ouput string\n",
    "    system_prompt=system_prompt,\n",
    "    skip_original_verification= True,\n",
    "    localization = llm_pipeline.LocStrategies.ORACLE,\n",
    "    # Options related with if will prompt LLM or gathered from saved results\n",
    "    skip_verification= True,\n",
    "    only_verify = False,\n",
    "    only_get_location = False,\n",
    "    only_get_assert_candidate = False,\n",
    ")\n",
    "\n",
    "run_options.examples_to_augment_prompt_type=llm_pipeline.ExampleStrategies.DYNAMIC # \"RANDOM\", \"DYNAMIC\", \"NONE\", \"EMBEDDED\" \"TFIDF\" all options     \n",
    "run_options.examples_weight_of_error_message = 0.25 # Wieght of the erro message in case of DYNAMIC in relantion with code                                 \n",
    "run_options.number_examples_to_add=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df46ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline\n",
      "Laurel Better\n",
      "LEN assertion groups 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Groups: cost (Active Cores:6) (seq):   0%|          | 0/448 [00:00<?, ?it/s]/home/ricostynha/.local/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ricostynha/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_bert_hyphen_v2_hyphen_qk_hyphen_post_hyphen_norm/3baf9e3ac750e76e8edd3019170176884695fb94/configuration_bert.py:29: UserWarning: optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\n",
      "  warnings.warn(\"optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\")\n",
      "Eval Groups: cost (Active Cores:6) (seq): 100%|██████████| 448/448 [00:24<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Prices Model name: cost Model id: us.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "Statistic                               Value               \n",
      "========================================\n",
      "Total Prompts                           448                 \n",
      "Total Chars Prompted                    3202623             \n",
      "Total Chars Response                    323456              \n",
      "Total Tokens Input                      1067541.00          \n",
      "Total Tokens Output                     107818.67           \n",
      "Total Tokens Output Reason              0.00                \n",
      "Cost Input ($)                          1.067541            \n",
      "Cost Output ($)                         0.539093            \n",
      "Cost Output Reason($)                   0.000000            \n",
      "Total Cost ($)                          1.606634            \n",
      "========================================\n",
      "LLM\n",
      "LEN assertion groups 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Groups: cost (Active Cores:6) (seq): 100%|██████████| 448/448 [00:16<00:00, 27.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Prices Model name: cost Model id: us.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "Statistic                               Value               \n",
      "========================================\n",
      "Total Prompts                           896                 \n",
      "Total Chars Prompted                    4724112             \n",
      "Total Chars Response                    327040              \n",
      "Total Tokens Input                      1574704.00          \n",
      "Total Tokens Output                     109013.33           \n",
      "Total Tokens Output Reason              0.00                \n",
      "Cost Input ($)                          1.574704            \n",
      "Cost Output ($)                         0.545067            \n",
      "Cost Output Reason($)                   0.000000            \n",
      "Total Cost ($)                          2.119771            \n",
      "========================================\n",
      "LLM_EXAMPLE\n",
      "LEN assertion groups 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Groups: cost (Active Cores:6) (seq): 100%|██████████| 448/448 [00:31<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Prices Model name: cost Model id: us.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "Statistic                               Value               \n",
      "========================================\n",
      "Total Prompts                           896                 \n",
      "Total Chars Prompted                    6716655             \n",
      "Total Chars Response                    327040              \n",
      "Total Tokens Input                      2238885.00          \n",
      "Total Tokens Output                     109013.33           \n",
      "Total Tokens Output Reason              0.00                \n",
      "Cost Input ($)                          2.238885            \n",
      "Cost Output ($)                         0.545067            \n",
      "Cost Output Reason($)                   0.000000            \n",
      "Total Cost ($)                          2.783952            \n",
      "========================================\n",
      "HYBRID\n",
      "LEN assertion groups 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Groups: cost (Active Cores:6) (seq): 100%|██████████| 448/448 [00:31<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Prices Model name: cost Model id: us.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "Statistic                               Value               \n",
      "========================================\n",
      "Total Prompts                           896                 \n",
      "Total Chars Prompted                    6739519             \n",
      "Total Chars Response                    327040              \n",
      "Total Tokens Input                      2246506.33          \n",
      "Total Tokens Output                     109013.33           \n",
      "Total Tokens Output Reason              0.00                \n",
      "Cost Input ($)                          2.246506            \n",
      "Cost Output ($)                         0.545067            \n",
      "Cost Output Reason($)                   0.000000            \n",
      "Total Cost ($)                          2.791573            \n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_cost_stub = create_llm(\"cost\",\"cost_stub_almost_real\")\n",
    "\n",
    "print(\"Pipeline\")\n",
    "print(\"Laurel Better\")\n",
    "run_options.localization = llm_pipeline.LocStrategies.LAUREL_BETTER\n",
    "llm_pipeline.evaluate_all(llm_cost_stub, global_options, run_options)\n",
    "llm_cost_stub.get_cost_statistics(llm_configurations.MODEL_REGISTRY[\"claude-haiku-4.5\"])\n",
    "llm_cost_stub.reset_all_measurement()\n",
    "\n",
    "print(\"LLM\")\n",
    "run_options.localization = llm_pipeline.LocStrategies.LLM\n",
    "llm_pipeline.evaluate_all(llm_cost_stub, global_options, run_options)\n",
    "llm_cost_stub.get_cost_statistics(llm_configurations.MODEL_REGISTRY[\"claude-haiku-4.5\"])\n",
    "llm_cost_stub.reset_all_measurement()\n",
    "\n",
    "print(\"LLM_EXAMPLE\")\n",
    "run_options.examples_to_augment_prompt_type_pos = llm_pipeline.ExampleStrategies.DYNAMIC\n",
    "run_options.examples_weight_of_error_message_pos = 0.25\n",
    "run_options.number_examples_to_add_pos = 3\n",
    "run_options.localization = llm_pipeline.LocStrategies.LLM_EXAMPLE\n",
    "llm_pipeline.evaluate_all(llm_cost_stub, global_options, run_options)\n",
    "llm_cost_stub.get_cost_statistics(llm_configurations.MODEL_REGISTRY[\"claude-haiku-4.5\"])\n",
    "llm_cost_stub.reset_all_measurement()\n",
    "\n",
    "print(\"HYBRID\")\n",
    "run_options.localization = llm_pipeline.LocStrategies.HYBRID\n",
    "llm_pipeline.evaluate_all(llm_cost_stub, global_options, run_options)\n",
    "llm_cost_stub.get_cost_statistics(llm_configurations.MODEL_REGISTRY[\"claude-haiku-4.5\"])\n",
    "llm_cost_stub.reset_all_measurement()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method that wouls andser the complete program on output\n",
      "LEN assertion groups 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Groups: cost (Active Cores:6) (seq): 100%|██████████| 448/448 [00:16<00:00, 26.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Prices Model name: cost Model id: us.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "Statistic                               Value               \n",
      "========================================\n",
      "Total Prompts                           448                 \n",
      "Total Chars Prompted                    3280573             \n",
      "Total Chars Response                    554985              \n",
      "Total Tokens Input                      1093524.33          \n",
      "Total Tokens Output                     184995.00           \n",
      "Total Tokens Output Reason              0.00                \n",
      "Cost Input ($)                          1.093524            \n",
      "Cost Output ($)                         0.924975            \n",
      "Cost Output Reason($)                   0.000000            \n",
      "Total Cost ($)                          2.018499            \n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_cost_stub = create_llm(\"cost\",\"cost_stub_response_dafnybench\")\n",
    "print(\"Method that would answer the complete program on the output\")\n",
    "run_options.localization = llm_pipeline.LocStrategies.ORACLE\n",
    "\n",
    "# compare also with examples on the prompt\n",
    "run_options.examples_to_augment_prompt_type=llm_pipeline.ExampleStrategies.DYNAMIC # \"RANDOM\", \"DYNAMIC\", \"NONE\", \"EMBEDDED\" \"TFIDF\" all options     \n",
    "run_options.examples_weight_of_error_message = 0.25 # Wieght of the erro message in case of DYNAMIC in relantion with code                                 \n",
    "run_options.number_examples_to_add=3 \n",
    "\n",
    "llm_pipeline.evaluate_all(llm_cost_stub, global_options, run_options)\n",
    "llm_cost_stub.get_cost_statistics(llm_configurations.MODEL_REGISTRY[\"claude-haiku-4.5\"])\n",
    "llm_cost_stub.reset_all_measurement()\n",
    "# Can be tested by only performing the inference prompt, but assuming that llm returns a response with full prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
